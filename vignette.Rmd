---
title: "hw2 package"
author: "Yixin Shen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# hw2
There are three function in this hw2 package:

olve_ols(): a function solves a linear system using Gauss-Seidel or Jacobi method,
allows user to specify how many cores to use for parallel implementation.

algo_leverage(): a function implements algorithmic leveraging for linear 
regression using uniform and leverage score based subsampling of rows.

elnet_coord(): a function fits elastic net to data using coordinate descent algorithm.

## Download and install the package

To download and install the package, use `devtools`: (need to download this if you don't have it)

```{r}
library(devtools)
install_github("https://github.com/AnnyShen55/hw2")
library(hw2)
```

Then you will be able to use these three functions in this package.

#Solving Linear System by Gauss-Seidel and Jacobi, parallel allowed.

`solve_ols()`: This function solves a linear system by Gauss-Seidel and Jacobi, parallel allowed for both of them. For the algorithm parameter, users can choose to use "Gauss-Seidel", "Jacobi", "parallel Gauss-Seidel", and "parallel Jacobi". By default, it's "Gauss-Seidel". Or instead, users can use numbers 1, 2, 3, and 4 respectively. This function would tell you when the algorithm doesn't converge. 

```{r}
n = 100
D = diag(rep(1, n))
U = rbind(cbind(rep(0, n - 1), diag(rep(-1, n - 1))), rep(0, n))
L = t(U)
 v = rep(c(1, 0), as.integer(n / 2))
#print(L+D+U)
 b = (L + D + U) %*% v
 X = L + D + U
solve_ols(X,b)
```


```{r}
n = 100
D = diag(rep(2, n))
U = rbind(cbind(rep(0, n - 1), diag(rep(-1, n - 1))), rep(0, n))
L = t(U)
 v = rep(c(1, 0), as.integer(n / 2))
#print(L+D+U)
 b = (L + D + U) %*% v
 X = L + D + U
solve_ols(X,b)
```

```{r}
n = 100
D = diag(rep(3, n))
U = rbind(cbind(rep(0, n - 1), diag(rep(-1, n - 1))), rep(0, n))
L = t(U)
 v = rep(c(1, 0), as.integer(n / 2))
#print(L+D+U)
 b = (L + D + U) %*% v
 X = L + D + U
solve_ols(X,b)
```

#Algorithmic Leveraging

`algo_leverage`: This algorithm attempts to approximate the linear regression coefficient beta in a dataset of sample size n using only a randomly selected subset of size r much more smaller than n. Users can specify the the algorithm to be "uniform" or "leverage", and the default setting is "leverage". Or instead, users can use numbers 1 or 2, respectively. The number of the subsampling of the rows can be specified, by default it's 100. 

```{r}
n = 500
X = matrix(rt(n, 6),n)
eps = rnorm(n)#error term
Y = - X + eps
algo_leverage(X,Y)
```

```{r}
n = 500
X = matrix(rt(n, 6),n)
eps = rnorm(n)
Y = - X + eps
algo_leverage(X,Y,1)
```

#Elastic Net by Coordinate Descent Algorithm

`elnet_coord()`: It gives the beta estimate after fitting elastic net to data using coordinate descent algorithm. Users can specify the hyperparameter lambda and alpha of the elastic net. By default, lambda = 0.5, alpha = 0, which is a ridge regression. 

```{r}
p = 20
 n  = 50
beta_0 = rep(0, p)
alpha = c(0, 0.5, 1)
beta_partial = c(2, 0,-2, 0, 1, 0, -1, 0)
beta = c(beta_partial, rep(0, 12))
corr_mat = diag(20)
corr_mat[1, 2] = 0.8
corr_mat[2, 1] = 0.8
corr_mat[5, 6] = 0.8
corr_mat[6, 5] = 0.8
eps = matrix(rnorm(n), n)
X = MASS::mvrnorm(n, rep(0, p), Sigma = corr_mat)
Y = X %*% beta + eps
elnet_coord(X,Y)
elnet_coord(X,Y, alpha = 0.5)
elnet_coord(X,Y, alpha = 1)
```




